<!DOCTYPE html>
<html>
<head>
  <link rel="icon" type="image/png" href="favicon.png">
  <title>Phoneme-Feature Analysis - Help</title>
  <link rel="stylesheet" href="style.css">
</head>
<body class="feature-help">
  <h1>Okay but what's going on?</h1>
  
  <h5>Glad you asked. Buckle up, abscraction awaits!</h5>
  <h2>What do we mean by "sound features"?</h2>
  <p>Human vocal tract consists of several muscles working together to produce sound. Thanks to varying 
    combinations of muscles involved in the production of any particular sound, sound segments have different features.
    Now many of the features are simply binary.
    For example, humans sometimes employ their vocal cords to produce sounds like /g/ or /d/ but "turn them off" 
    to produce sounds like /k/ or /t/. Ans so, /g/ and /d/ share the value for a feature named "voice",
     where the value for /g/ and /d/ is 1 (or +)&mdash;while for /k/ and /t/ it is 0 (or -).</p>

     <div style="text-align: center;">
        <img src="imag.png" alt="Vocal tract diagram" width="200">
  <img src="image.png" alt="Vocal tract diagram" width="200">
   <img src="img.png" alt="Vocal tract diagram" width="200">
</div>

     <h2>How many "features" does a sound segment have?</h2>
     <p>This is actually a complex question. First of, some features are non-binary. For instance, one cannot  
        "turn on" or "turn off" their place of articulation. There simply are more than two places in the tract 
        where specific sound segments are produced. For instance, sounds like /p/ or /m/ are articulated at lips, 
        /t/ or /l/ at the front of the mouth, and /k/ and /&#x14b/ at the back of the tongue. This is where we can 
        capitalise on the finiteness: because there is a constant number of places of articulation, we can simply 
        make them a feature each and only mark one as operative. By this definition, every sound can have up to 25 binary features!</p>

    <h2>How many different sounds can humans produce?</h2>
    <p>It is not 25<sup>2</sup>, sorry to disappoint!  First, not all features always specify a sound. 
    Sometimes a feature just doesn't matter for a production of a particular sound, and the position in the vector would 
    therefore be neither 0 nor one but rather something as a "null". (An attentive reader may notice that this means the vector is not exactly binary.
  A simple workaround is to have two equal length specification vectors instead: one to
  specify which features are active and one to specify the now properly binary value of
  active features.) For instance, when producing the 
    sound /n/, all air comes out through the nose, and so any feature that has to 
    do with the position of the tongue or lips, such as [rnd] for lip rounding, will remain unspecified. 
Another hurdle is that not all features are completely independent. For instance, different kinds of vowels can be specified around 
    the position of the tongue&mdash;i.e. [high] or [low]. However, when the tongue is positioned high, it cannot at the 
    same time be positioned low. At the same time, not positioning the tongue high does not necessarily imply
positioning it low&mdash;it can be positioned in the middle&mdash;and so both features are still necessary to
specify the sound.</p>
    
    

<h2>Why bother?</h2>
<p>So, the question remaing why try so hard to specify sound segments as vectors of binary value. This is because 
    sounds that share values for specific features form classes. For instance /p/, /k/, and /t/ are all "voiceless" and 
    /b/, /g/, and /d/ are "voiced". Different classes of sounds tend to behave alike in different phonological
    environments. For instance /p/, /k/, and /t/&mdash;but not /b/, /g/, and /d/&mdash; get aspirated (pronounced
    with a little bit of an extra air) at the beginning of syllables (cf. /pump/ versus /bump/). Binary vectors
    allow us to extract features sound segments have in common and identify classes more neatly.
</p>

<h2>Where is the maths?</h2>
<p>Let me demonstrate the problem on a simple set of vectors of only 3 values. Consider 3 binary vectors 
    representing sound segments A, B, and C such that A=[1,0,0], B=[0,1,1], and C=[1,0,1]. Say that you see segments B and C behaving alike in a certain sounds
    environments. Now notice that vectors B and C, but not A, share a value for the third feature. This means that it
    is likely the third feature that conditions the observed behaviour&mdash;simply because when this value changes,
    the observed behaviour changes too! We say that the third feature is the "minimum feature specification" of segments
    B and C in a sound inventory consisting of A, B, and C. You can also work the problem the other way around: suppose
    you know that the third feature conditions a certain behaviour. Given the inventory, you can easily predict
    that segments B and C will likely show this behaviour but A will not.
</p>

<h2>What does this mean in practice?</h2>
<p>This problem gets more interesting when it is not a single feature but rather a 
    combination of features that specifies a class. For instance, it is not all voiceless sound
segments that get aspirated at the beginning of syllables in English. Voiceless *stops*
like /p/ or /t/ get aspirated, but voiceless continuants like /f/ or /s/ do not (it would
actually be quite difficult to aspirate them). So, looking for the right linear combination
to specify a class of sound segments that get affected by various rules in various languages
is a part of the fun. Now as humans learn their first-ever language, their
brains learn to identify groups of sounds by their features and their vocal tract muscles learn thr groups'
behaviours in various environments (recall examples with aspirating voiceless stops at the beginning od syllables).
As they learn other languages later on, they can no longer unlearn these behaviours and will reproduce them on
lexical items where they no longer apply. This is what we call "speaking with an accent". (Some people are very good 
at learning the behaviours of sound segment classes in other languages, which allows them to "speak without an
accent" in langauges they learn&mdash;or become stand-up comedians.)</p>

<h2>How is this interesting?</h2>
<p>Notice that that this problem is only
solvable thanks to its finite nature: a finite universe of features, finite inventories of sound segments,
finite sets of target segments. In the above problem, we can only say that the third feature uniquely 
identifies segments B and C *in the inventory* of A, B, and C. This also means that the human brain is performing
quite a large computation every time we speak a word. It also makes one think about all the computations
that cannot be unlearnt and the subsequent limitations of overriding. And, on a higher level, how defined subspaces
help in the making sense of infinity.
</p>
<footer class="footer">
  <div class="footer-left">
     made with ü§ç for language &copy; 2025
  </div>
  <div class="footer-right">
    <img src="signature.png" alt="Signature" class="footer-signature">

  </div>
</footer>
</body>
</html>